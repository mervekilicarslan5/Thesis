{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (2.1.1)\n",
            "Requirement already satisfied: torchvision in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (0.16.1)\n",
            "Requirement already satisfied: torchaudio in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (2.1.1)\n",
            "Requirement already satisfied: sympy in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: filelock in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: jinja2 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: fsspec in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torch) (2023.12.1)\n",
            "Requirement already satisfied: numpy in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torchvision) (1.26.2)\n",
            "Requirement already satisfied: requests in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from torchvision) (10.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision torchaudio --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in /Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages (1.26.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6X_gVBsTLeY",
        "outputId": "141c3d4c-c5fe-42e2-a1e2-23e0e645f9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['qnnpack', 'none']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.quantization import QuantStub, DeQuantStub, convert\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "print(torch.backends.quantized.supported_engines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save weights in binary format for compatibility with CPP library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "TFj01u-9a24m"
      },
      "outputs": [],
      "source": [
        "#Save weights compatible with the cpp library\n",
        "def save_weights_compatible_with_cpp(model, filepath):\n",
        "    with open(filepath, 'wb') as f:\n",
        "        # First, write the total number of parameters\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        f.write(np.array([total_params], dtype=np.int32).tobytes())\n",
        "\n",
        "        # Now, write each parameter as a flat array of floats\n",
        "        for param_tensor in model.parameters():\n",
        "            # Ensure it's a CPU tensor and then convert to numpy\n",
        "            param_numpy = param_tensor.data.cpu().numpy().ravel().astype(np.float32)\n",
        "            f.write(param_numpy.tobytes())\n",
        "\n",
        "\n",
        "def save_quantized_weights_compatible_with_cpp(model, filepath):\n",
        "    with open(filepath, 'wb') as f:\n",
        "        # Write each quantized parameter as a flat array of integers, along with scale and zero-point\n",
        "        for name, module in model.named_modules():\n",
        "            if hasattr(module, 'weight'):\n",
        "                # Extract scale and zero-point\n",
        "                scale = np.array([module.weight().q_scale()], dtype=np.float32)\n",
        "                zero_point = np.array([module.weight().q_zero_point()], dtype=np.int32)\n",
        "\n",
        "                # Extract and convert weights to int8\n",
        "                weight = module.weight().int_repr().cpu().numpy().ravel().astype(np.int8)\n",
        "\n",
        "                # Write scale, zero-point, and weight to file\n",
        "                f.write(scale.tobytes())\n",
        "                f.write(zero_point.tobytes())\n",
        "                f.write(weight.tobytes())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kyuiTQ8AbXZk"
      },
      "outputs": [],
      "source": [
        "# Prepare MNIST dataset with transformations\n",
        "transform_mnist = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "train_dataset_mnist = datasets.MNIST(root='./data', train=True, transform=transform_mnist, download=True)\n",
        "train_loader_mnist = DataLoader(dataset=train_dataset_mnist, batch_size=64, shuffle=True)\n",
        "test_dataset_mnist = datasets.MNIST(root='./data', train=False, transform=transform_mnist, download=True)\n",
        "test_loader = DataLoader(dataset=test_dataset_mnist, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "2aw4g8QkIo1y",
        "outputId": "f3be8787-ea8a-4a5e-8b67-afa0f476a7f4"
      },
      "outputs": [],
      "source": [
        "#Basic LeNET architecture competible with CPP library\n",
        "class LeNet(nn.Module):\n",
        "# Add quant and dequant layers for post-training quantization\n",
        "    def __init__(self, quantization = False):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, padding=2),  # Padding is 2 to match the C++ code\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5),  # No padding here to match the C++ code\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Calculate the size of the flattened feature maps.\n",
        "        # With padding=2 in the first conv layer and input images being 32x32,\n",
        "        # the output of the second conv layer would be 16x5x5 feature maps\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(16*5*5, 120),  # Adjusted for the output of 5x5 feature maps\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10)\n",
        "        )\n",
        "        self.quantization =  quantization\n",
        "        # Quantization and Dequantization stubs\n",
        "        if  quantization:\n",
        "          self.quant = QuantStub()\n",
        "          self.dequant = DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.quantization:\n",
        "          x = self.quant(x)  # Quantize input\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.shape[0], -1) #Dont use view\n",
        "        x = self.fc(x)\n",
        "        if self. quantization:\n",
        "          x = self.dequant(x)  # Dequantize output\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "D0BizpvEf5IX"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader, cuda=False, q=False):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Train LeNet on MNIST\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader_mnist):\n",
        "            images, labels = images, labels\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Print loss every 100 batches\n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader_mnist)}], Loss: {total_loss/100:.4f}\")\n",
        "                total_loss = 0.0\n",
        "    print('Finished Training')\n",
        "\n",
        "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if cuda:\n",
        "              inputs = inputs\n",
        "              labels = labels\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.1 No quantization training\n",
        "Here the training is done without any quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [500/938], Loss: 11.4775\n",
            "Epoch [2/10], Step [500/938], Loss: 3.6309\n",
            "Epoch [3/10], Step [500/938], Loss: 1.7005\n",
            "Epoch [4/10], Step [500/938], Loss: 1.2263\n",
            "Epoch [5/10], Step [500/938], Loss: 1.0180\n",
            "Epoch [6/10], Step [500/938], Loss: 0.8547\n",
            "Epoch [7/10], Step [500/938], Loss: 0.7332\n",
            "Epoch [8/10], Step [500/938], Loss: 0.6244\n",
            "Epoch [9/10], Step [500/938], Loss: 0.5549\n",
            "Epoch [10/10], Step [500/938], Loss: 0.4930\n",
            "Finished Training\n",
            "Test Accuracy: 97.0%\n"
          ]
        }
      ],
      "source": [
        "# Assuming the LeNet model and MNIST DataLoaders are initialized\n",
        "lenet = LeNet()\n",
        "\n",
        "# Train the model\n",
        "train(lenet, train_loader_mnist, cuda=True, q=False)\n",
        "\n",
        "# Test the model\n",
        "accuracy = test(lenet, test_loader, cuda=True)\n",
        "print(f'Test Accuracy: {accuracy}%')\n",
        "\n",
        "# Save the model weights\n",
        "save_weights_compatible_with_cpp(lenet, 'lenet_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xtJW86fJcw1Z"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n    # Save the quantized model\\n    #model_save_path = \\'/content/drive/My Drive/lenet_mnist_weights_quantized.pth\\'\\n    #torch.save(lenet.state_dict(), model_save_path)\\n    #print(f\\'Quantized model weights saved to {model_save_path}\\')\\n\\n    # Save the weights of the model in a format compatible with C++\\n    #save_path = \\'/content/drive/My Drive/lenet_weights_quantized.pth\\'\\n    #save_weights_compatible_with_cpp(lenet, save_path)\\n\\nquantization_info = {}\\n\\nfor name, module in lenet.named_modules():\\n    if isinstance(module, torch.nn.quantized.Conv2d) or isinstance(module, torch.nn.quantized.Linear):\\n        layer_info = {\\n            \"scale\": float(module.scale),\\n            \"zero_point\": int(module.zero_point)\\n            }\\n\\n        if isinstance(module, torch.nn.quantized.Conv2d):\\n            layer_info.update({\\n                    \"kernel_size\": module.kernel_size,\\n                    \"stride\": module.stride,\\n                    \"padding\": module.padding\\n                })\\n        elif isinstance(module, torch.nn.quantized.Linear):\\n            layer_info.update({\\n                    \"in_features\": module.in_features,\\n                    \"out_features\": module.out_features\\n                })\\n\\n        quantization_info[name] = layer_info\\n\\n    # Optionally, save the information to a file\\n    #with open(\"quantization_info.json\", \"w\") as file:\\n    #    json.dump(quantization_info, file, indent=4)\\n\\n'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "'''\n",
        "    # Save the quantized model\n",
        "    #model_save_path = '/content/drive/My Drive/lenet_mnist_weights_quantized.pth'\n",
        "    #torch.save(lenet.state_dict(), model_save_path)\n",
        "    #print(f'Quantized model weights saved to {model_save_path}')\n",
        "\n",
        "    # Save the weights of the model in a format compatible with C++\n",
        "    #save_path = '/content/drive/My Drive/lenet_weights_quantized.pth'\n",
        "    #save_weights_compatible_with_cpp(lenet, save_path)\n",
        "\n",
        "quantization_info = {}\n",
        "\n",
        "for name, module in lenet.named_modules():\n",
        "    if isinstance(module, torch.nn.quantized.Conv2d) or isinstance(module, torch.nn.quantized.Linear):\n",
        "        layer_info = {\n",
        "            \"scale\": float(module.scale),\n",
        "            \"zero_point\": int(module.zero_point)\n",
        "            }\n",
        "\n",
        "        if isinstance(module, torch.nn.quantized.Conv2d):\n",
        "            layer_info.update({\n",
        "                    \"kernel_size\": module.kernel_size,\n",
        "                    \"stride\": module.stride,\n",
        "                    \"padding\": module.padding\n",
        "                })\n",
        "        elif isinstance(module, torch.nn.quantized.Linear):\n",
        "            layer_info.update({\n",
        "                    \"in_features\": module.in_features,\n",
        "                    \"out_features\": module.out_features\n",
        "                })\n",
        "\n",
        "        quantization_info[name] = layer_info\n",
        "\n",
        "    # Optionally, save the information to a file\n",
        "    #with open(\"quantization_info.json\", \"w\") as file:\n",
        "    #    json.dump(quantization_info, file, indent=4)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K4vMhAlaqzo"
      },
      "source": [
        "**1.2 post-training quantization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kkWLKlyH3B5",
        "outputId": "5f63de33-ea8d-405b-b738-26ce18862ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv.0 - Scale: 0.07743089646100998, Zero Point: 121\n",
            "conv.3 - Scale: 0.1501348316669464, Zero Point: 107\n",
            "fc.0 - Scale: 0.13934408128261566, Zero Point: 110\n",
            "fc.2 - Scale: 0.08970557153224945, Zero Point: 108\n",
            "fc.4 - Scale: 0.1313752382993698, Zero Point: 134\n",
            "quant - Scale: tensor([0.0078]), Zero Point: tensor([128])\n",
            "Accuracy of the network on the test images: 97.04%\n"
          ]
        }
      ],
      "source": [
        "def load_model(quantized_model, model):\n",
        "    \"\"\" Loads in the weights into an object meant for quantization \"\"\"\n",
        "    state_dict = model.state_dict()\n",
        "    model = model.to('cpu')\n",
        "    quantized_model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "#torch.save(lenet.state_dict(), 'lenet_full_precision.pth')\n",
        "torch.backends.quantized.engine = 'qnnpack'\n",
        "\n",
        "# Instantiate the LeNet model\n",
        "lenet_quant = LeNet(quantization=True)\n",
        "load_model(lenet_quant, lenet)\n",
        "#lenet_quant.load_state_dict(lenet.state_dict())\n",
        "lenet_quant.eval()\n",
        "# Fuse Conv, bn and relu\n",
        "#net.fuse_model()\n",
        "\n",
        "# Specify quantization configuration\n",
        "lenet_quant.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Prepare the model for static quantization\n",
        "torch.quantization.prepare(lenet_quant, inplace=True)\n",
        "\n",
        "# Calibrate the model and collect statistics\n",
        "for inputs, _ in train_loader_mnist:\n",
        "    lenet_quant(inputs)\n",
        "\n",
        "# Convert to a quantized model\n",
        "torch.quantization.convert(lenet_quant, inplace=True)\n",
        "# Print scale and zero-point information\n",
        "for name, module in lenet_quant.named_modules():\n",
        "    if hasattr(module, 'scale') and hasattr(module, 'zero_point'):\n",
        "        print(f\"{name} - Scale: {module.scale}, Zero Point: {module.zero_point}\")\n",
        "\n",
        "    # Training process\n",
        "score = test(lenet_quant, test_loader, cuda=True)\n",
        "print(f'Accuracy of the network on the test images: {score}%')\n",
        "save_quantized_weights_compatible_with_cpp(lenet_quant, 'postquantization_lenet.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.3 Quantization Aware Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "sWWrH7_CZxdh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [500/938], Loss: 0.1390\n",
            "Epoch [2/10], Step [500/938], Loss: 0.1381\n",
            "Epoch [3/10], Step [500/938], Loss: 0.1464\n",
            "Epoch [4/10], Step [500/938], Loss: 0.1353\n",
            "Epoch [5/10], Step [500/938], Loss: 0.1295\n",
            "Epoch [6/10], Step [500/938], Loss: 0.1278\n",
            "Epoch [7/10], Step [500/938], Loss: 0.1221\n",
            "Epoch [8/10], Step [500/938], Loss: 0.1154\n",
            "Epoch [9/10], Step [500/938], Loss: 0.1200\n",
            "Epoch [10/10], Step [500/938], Loss: 0.1109\n",
            "Finished Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mervekilicarslan/Library/Python/3.9/lib/python/site-packages/torch/ao/quantization/utils.py:317: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the fused and quantized network (trained quantized) on the test images:- INT89.8\n"
          ]
        }
      ],
      "source": [
        "torch.backends.quantized.engine = 'qnnpack'\n",
        "qnet = LeNet(quantization=True)\n",
        "qnet.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
        "torch.quantization.prepare_qat(qnet, inplace=True)\n",
        "train(qnet, train_loader_mnist, cuda=True)\n",
        "qnet = qnet.cpu()\n",
        "torch.quantization.convert(qnet, inplace=True)\n",
        "score = test(qnet, test_loader, cuda=False)\n",
        "print(f'Accuracy of the fused and quantized network (trained quantized) on the test images:- INT8{score}')\n",
        "save_quantized_weights_compatible_with_cpp(qnet, 'qat_lenet.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
